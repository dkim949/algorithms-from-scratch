{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Multi Layer Perceptron\n","\n","## 모델 구조\n","\n","- **입력층**: `(5, 3)`  → 5개의 샘플, 각 샘플당 3개의 특징\n","- **은닉층 1**: `(3, 4)`  → 입력 크기 3, 출력 크기 4  \n","  - **바이어스 크기**: `(4,)` → 은닉층 1의 4개 노드 각각에 바이어스 적용\n","- **은닉층 2**: `(4, 3)`  → 입력 크기 4, 출력 크기 3  \n","  - **바이어스 크기**: `(3,)` → 은닉층 2의 3개 노드 각각에 바이어스 적용\n","- **출력층**: `(3, 2)`  → 입력 크기 3, 출력 크기 2  \n","  - **바이어스 크기**: `(2,)` → 출력층의 2개 노드 각각에 바이어스 적용\n","\n","---\n","\n","- **최종 출력**: `(5, 2)` → 5개의 샘플, 각 샘플당 2개의 예측 값\n","\n","---\n","\n","### 바이어스 크기 설명\n","\n","각 층의 **바이어스(bias) 크기**는 해당 층의 **노드 수와 동일**합니다. 이는 **각 노드가 독립적으로 하나의 바이어스를 가지며, 이 바이어스를 더해 출력**을 조정하기 때문입니다.  \n","이 구조를 통해 각 노드는 자신의 바이어스를 활용하여 **출력을 개별적으로 조정**할 수 있습니다.\n"]},{"cell_type":"markdown","metadata":{},"source":["## 초기 파라미터 설정(5x3 matrix)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"text/plain":["array([[ 1.76405235,  0.40015721,  0.97873798],\n","       [ 2.2408932 ,  1.86755799, -0.97727788],\n","       [ 0.95008842, -0.15135721, -0.10321885],\n","       [ 0.4105985 ,  0.14404357,  1.45427351],\n","       [ 0.76103773,  0.12167502,  0.44386323]])"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","\n","batch_size = 5    # 데이터 레코드 수 (행 수)\n","input_size = 3    # 입력 특징 수 (열 수)\n","\n","hidden_size1 = 4  # 첫 번째 은닉층 노드 수\n","hidden_size2 = 3  # 두 번째 은닉층 노드 수\n","output_size = 2   # 출력 노드 수 (각 레코드당 2개의 예측값)\n","\n","# 랜덤 입력 데이터 생성 (배치 크기 x 입력 크기)\n","np.random.seed(0)  # 결과 재현성을 위해 시드 고정\n","X = np.random.randn(batch_size, input_size)  # (5, 3) 크기의 입력 데이터\n","y_true = np.random.randn(batch_size, output_size)  # 실제값 생성\n","X"]},{"cell_type":"markdown","metadata":{},"source":["## Init. Weights and Biases"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["array([[-1.45436567,  0.04575852, -0.18718385,  1.53277921],\n","       [ 1.46935877,  0.15494743,  0.37816252, -0.88778575],\n","       [-1.98079647, -0.34791215,  0.15634897,  1.23029068]])"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# 입력층 -> 첫 번째 은닉층\n","W1 = np.random.randn(input_size, hidden_size1)  # (3, 4)  # random normal => 가중치 초기화시 사용\n","B1 = np.random.randn(hidden_size1)              # (4,)\n","\n","# 첫 번째 은닉층 -> 두 번째 은닉층\n","W2 = np.random.randn(hidden_size1, hidden_size2)\n","B2 = np.random.randn(hidden_size2)\n","\n","# 두 번째 은닉층 -> 출력층\n","W3 = np.random.randn(hidden_size2, output_size)\n","B3 = np.random.randn(output_size)\n","\n","W1"]},{"cell_type":"markdown","metadata":{},"source":["## 순전파 (Forward Pass)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Input (X):\n"," [[ 1.76405235  0.40015721  0.97873798]\n"," [ 2.2408932   1.86755799 -0.97727788]\n"," [ 0.95008842 -0.15135721 -0.10321885]\n"," [ 0.4105985   0.14404357  1.45427351]\n"," [ 0.76103773  0.12167502  0.44386323]]\n","\n","Hidden Layer 1 Output:\n"," [[-0.99125299 -0.52637527 -0.31686264  0.98672627]\n"," [ 0.98952236  0.3315719  -0.166746   -0.44149455]\n"," [-0.1948163  -0.31977234 -0.50315354  0.39279984]\n"," [-0.96826531 -0.69220595 -0.09700809  0.84605437]\n"," [-0.54050289 -0.45268728 -0.31793322  0.50501049]]\n","\n","Hidden Layer 2 Output:\n"," [[-0.56128338  0.99243674 -0.85716416]\n"," [-0.98632016 -0.9428962   0.97594679]\n"," [-0.90220597  0.88713785  0.34106319]\n"," [-0.27838456  0.98402977 -0.76363405]\n"," [-0.7073226   0.94689644 -0.24423053]]\n","\n","Output Layer Output:\n"," [[-0.71804473 -0.96015232]\n"," [-0.73381284 -0.96631912]\n"," [-0.93091971 -0.98502809]\n"," [-0.73629633 -0.95562263]\n"," [-0.86084464 -0.97554547]]\n"]}],"source":["# 1. 입력층에서 첫 번째 은닉층으로\n","hidden_layer1_input = np.dot(X, W1) + B1         # (5, 4) = (5, 3) * (3, 4) + (4,)\n","hidden_layer1_output = np.tanh(hidden_layer1_input)  # 활성화 함수 적용\n","\n","# 2. 첫 번째 은닉층에서 두 번째 은닉층으로\n","hidden_layer2_input = np.dot(hidden_layer1_output, W2) + B2  # (5, 3) = (5, 4) * (4, 3) + (3,)\n","hidden_layer2_output = np.tanh(hidden_layer2_input)          # 활성화 함수 적용\n","\n","# 3. 두 번째 은닉층에서 출력층으로\n","output_layer_input = np.dot(hidden_layer2_output, W3) + B3  # (5, 2) = (5, 3) * (3, 2) + (2,)\n","output_layer_output = np.tanh(output_layer_input)           # 활성화 함수 적용\n","\n","# 결과 출력\n","print(\"Input (X):\\n\", X)\n","print(\"\\nHidden Layer 1 Output:\\n\", hidden_layer1_output)\n","print(\"\\nHidden Layer 2 Output:\\n\", hidden_layer2_output)\n","print(\"\\nOutput Layer Output:\\n\", output_layer_output)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial Loss: 2.7300612145982535\n"]}],"source":["# 손실 계산 (Mean Squared Error)\n","loss = np.mean((output_layer_output - y_true) ** 2)\n","print(\"Initial Loss:\", loss)"]},{"cell_type":"markdown","metadata":{},"source":["# 역전파 (Backpropagation)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# 1. 출력층에서 두 번째 은닉층으로의 기울기\n","dL_doutput = 2 * (output_layer_output - y_true) / batch_size\n","d_output_dinput = 1 - np.tanh(output_layer_input) ** 2  # tanh 미분\n","dL_dinput3 = dL_doutput * d_output_dinput\n","\n","# 가중치와 바이어스에 대한 기울기 계산\n","dL_dW3 = np.dot(hidden_layer2_output.T, dL_dinput3)\n","dL_dB3 = np.sum(dL_dinput3, axis=0)\n","\n","# 2. 두 번째 은닉층에서 첫 번째 은닉층으로의 기울기\n","dL_dhidden2 = np.dot(dL_dinput3, W3.T)\n","d_hidden2_dinput = 1 - np.tanh(hidden_layer2_input) ** 2\n","dL_dinput2 = dL_dhidden2 * d_hidden2_dinput\n","\n","# 가중치와 바이어스에 대한 기울기 계산\n","dL_dW2 = np.dot(hidden_layer1_output.T, dL_dinput2)\n","dL_dB2 = np.sum(dL_dinput2, axis=0)\n","\n","# 3. 첫 번째 은닉층에서 입력층으로의 기울기\n","dL_dhidden1 = np.dot(dL_dinput2, W2.T)\n","d_hidden1_dinput = 1 - np.tanh(hidden_layer1_input) ** 2\n","dL_dinput1 = dL_dhidden1 * d_hidden1_dinput\n","\n","# 가중치와 바이어스에 대한 기울기 계산\n","dL_dW1 = np.dot(X.T, dL_dinput1)\n","dL_dB1 = np.sum(dL_dinput1, axis=0)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# 가중치와 바이어스 업데이트\n","learning_rate = 0.01  # 학습률 설정\n","\n","W1 -= learning_rate * dL_dW1\n","B1 -= learning_rate * dL_dB1\n","W2 -= learning_rate * dL_dW2\n","B2 -= learning_rate * dL_dB2\n","W3 -= learning_rate * dL_dW3\n","B3 -= learning_rate * dL_dB3"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Updated Loss: 2.725618925635357\n"]}],"source":["# 업데이트 후 손실 계산\n","hidden_layer1_input = np.dot(X, W1) + B1\n","hidden_layer1_output = np.tanh(hidden_layer1_input)\n","\n","hidden_layer2_input = np.dot(hidden_layer1_output, W2) + B2\n","hidden_layer2_output = np.tanh(hidden_layer2_input)\n","\n","output_layer_input = np.dot(hidden_layer2_output, W3) + B3\n","output_layer_output = np.tanh(output_layer_input)\n","\n","updated_loss = np.mean((output_layer_output - y_true) ** 2)\n","print(\"Updated Loss:\", updated_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# 함수로 구현"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.15"}},"nbformat":4,"nbformat_minor":2}

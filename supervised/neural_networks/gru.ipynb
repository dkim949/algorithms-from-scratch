{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Recurrent Unit (GRU)\n",
    "\n",
    "LSTM을 단순화한 구조로, 업데이트 게이트와 리셋 게이트만을 사용하여 효율적으로 시퀀스를 처리하는 순환 신경망\n",
    "\n",
    "## 모델 구조\n",
    "- 입력(x), 은닉 상태(h), 두 개의 게이트(z, r)\n",
    "\n",
    "### 순전파 수식\n",
    "```\n",
    "# 게이트 계산\n",
    "z_t = σ(W_z·[h_{t-1}, x_t] + b_z)    # 업데이트 게이트\n",
    "r_t = σ(W_r·[h_{t-1}, x_t] + b_r)    # 리셋 게이트\n",
    "\n",
    "# 상태 업데이트\n",
    "h̃_t = tanh(W·[r_t * h_{t-1}, x_t] + b)  # 후보 은닉 상태\n",
    "h_t = (1 - z_t) * h_{t-1} + z_t * h̃_t    # 최종 은닉 상태\n",
    "```\n",
    "\n",
    "### 게이트 메커니즘\n",
    "- **업데이트 게이트 (z)**:\n",
    "  - 이전 상태와 새로운 상태의 조합 비율 결정\n",
    "  - LSTM의 망각/입력 게이트 역할 통합\n",
    "\n",
    "- **리셋 게이트 (r)**:\n",
    "  - 이전 상태를 얼마나 무시할지 결정\n",
    "  - 새로운 입력과 이전 상태의 관련성 조절\n",
    "\n",
    "### 특징\n",
    "- LSTM 대비 단순한 구조\n",
    "  - 셀 상태(C) 제거\n",
    "  - 출력 게이트 제거\n",
    "  - 더 적은 파라미터\n",
    "\n",
    "## 장점\n",
    "- LSTM과 유사한 성능\n",
    "- 더 빠른 학습과 실행 속도\n",
    "- 더 적은 메모리 사용\n",
    "- 짧은 시퀀스에서 효과적\n",
    "\n",
    "## 한계점\n",
    "- LSTM 대비 복잡한 장기 의존성 학습이 다소 어려움\n",
    "- 매우 긴 시퀀스에서는 LSTM이 더 유리할 수 있음\n",
    "- 메모리 제어가 LSTM보다 덜 섬세함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
